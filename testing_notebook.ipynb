{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testgroud for OWR project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing modules for data analisis and creating predition models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import data, imread, resize\n",
    "from keras import models\n",
    "from numpy.lib.histograms import histogram\n",
    "from tqdm import tqdm, utils\n",
    "from random import shuffle\n",
    "from numpy import array, log\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import callbacks, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing python built in modules and those which are not related to data analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, dirname, abspath\n",
    "from os import error, listdir, path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization,  here the array size and location for logs are configured\n",
    "images size have to be equal determined during function initialization.\n",
    "<br>\n",
    "Variables:\n",
    "<br>\n",
    "`path: string` - path to the directory with images <br>\n",
    "`size: tuple` - tuple describing size of each image <br>\n",
    "`logDir: string` - directory where run logs are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRecgonition:\n",
    "    def __init__(self, path: str, size: tuple, logDir: str = \"logs\\\\fit\\\\\"):\n",
    "        self.path = path\n",
    "        self.size = size\n",
    "        self.logDir = f\"{logDir}log_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset defining takes no arguments but self. It builds an array structure for categorical crossentropy to process data. Number of categories is defined by number of directories inside data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def define_dataset(self) -> list:\n",
    "\n",
    "        self.dataset: list = []\n",
    "        self.categories = [ d for d in listdir(join(dirname(abspath(__file__)), self.path)) ]   # look for all subdirectories and name categories from them \n",
    "        # what the fuck? vvv\n",
    "        for cat in self.categories:\n",
    "            imgPath: str = join(join(dirname(abspath(__file__)), self.path), cat)\n",
    "            categoryIndex: int = self.categories.index(cat)\n",
    "            for img in tqdm(listdir(path)):\n",
    "                try:\n",
    "                    imgArray: list = imread(join(path, img))\n",
    "                    actualImg = resize(imgArray, self.size)\n",
    "                    self.dataset.append([actualImg, categoryIndex])\n",
    "                except Exception as e:\n",
    "                    exit(f\"Error in 'ImageRecoginiton.define_dataset' occured - stopped working: {e}\")\n",
    "        # end of what the fuck? ^^^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function parameters: <br>\n",
    "`batchSize: int` - size of the batch (amount) of files processed at the same time by the machine <br>\n",
    "`epochs: int` - number of times the shell has to be re-taught and how many iterations of script learning have to be done <br> \n",
    "`validationSplit: float` - percentage of images that are used to validate learining process. Images are chosen randomly and they are substracted from the main pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def modeling(\n",
    "        self, \n",
    "        batchSize: int,\n",
    "        epochs: int,\n",
    "        validationSplit: float):\n",
    "\n",
    "        shuffle(self.dataset)\n",
    "        trainingX, labelsY = [], []\n",
    "\n",
    "        for features, label in self.dataset:    # preparing dataset and its labels for further modeling\n",
    "            trainingX.append(features)\n",
    "            labelsY.append(label)\n",
    "        \n",
    "        trainingX: array = array(trainingX).reshape(-1, self.size[0], self.size[1], 3)  # reshape data as a numpy array\n",
    "        trainingX = trainingX/255.0\n",
    "\n",
    "        # actual modelingg, adding layers and creating models\n",
    "        self.modelSequential = Sequential()\n",
    "        \n",
    "        self.modelSequential.add(Conv2D(256, (3, 3), input_shape=trainingX.shape[1:]))\n",
    "        self.modelSequential.add(Activation('relu'))\n",
    "        self.modelSequential.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.modelSequential.add(Conv2D(256, (3, 3)))\n",
    "        self.modelSequential.add(Activation('relu'))\n",
    "        self.modelSequential.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.modelSequential.add(Flatten()).add(Dense(64)).add(Dense(7))\n",
    "        self.modelSequential.add(Activation('sigmoid'))\n",
    "        # TODO modeling in beta version\n",
    "        # TODO create several models to choose from as methods\n",
    "\n",
    "        self.modelSequential.compile(\n",
    "            loss=\"categorical_crossentropy\", \n",
    "            optimizer=\"adam\", \n",
    "            metrics=[\"accutracy\"])      # compiling model defined above\n",
    "        labelsYCategorical = utils.to_categorical(labelsY, num_classes=7)     # converting labelsY as a categories\n",
    "\n",
    "        tensorboard = callbacks.TensorBoard(log_dir = self.logDir, histogram_freq = 1)      # creating tensorboard\n",
    "\n",
    "        try:\n",
    "            self.modelSequential.fit(\n",
    "                trainingX, \n",
    "                labelsYCategorical, \n",
    "                batch_size = batchSize, \n",
    "                epochs = epochs, \n",
    "                validation_split = validationSplit,\n",
    "                callbacks = [tensorboard])\n",
    "        except Exception as e:\n",
    "            print(f\"Error unhandled: {e} while trying to fit model in 'ImageRecgonition.modeling\")\n",
    "            self.modelSequential.fit(\n",
    "                trainingX, \n",
    "                labelsYCategorical, \n",
    "                batch_size = batchSize, \n",
    "                epochs = epochs, \n",
    "                validation_split = validationSplit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function is calling predicting moel and validates it's accuracy at the end.\n",
    "Parameters: <br>\n",
    "`model` - model of neural network created to test application <br>\n",
    "`testIMG` - map of bits in PNG/WEBP/JPG format of a given size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calling_predictions(self, model, testImg):          # predicting tests\n",
    "        # preparing models and dimensions\n",
    "        imgArray = imread(testImg)\n",
    "        actualImg = resize(imgArray, self.size).reshape(-1, self.size[0], self.size[1], 3)\n",
    "        prediction = model.predict([actualImg])      # pick a test sample\n",
    "        return self.categories[int(prediction[0][0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image size (resoltion) error hanlding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def size(self) -> int:\n",
    "        try:\n",
    "            return len(self.dataset)\n",
    "        except UnboundLocalError as e:\n",
    "            return f\"Error in 'ImageRecognition.size' - dataset not created yet: {e}. Use 'ImageRecognition.define_dataset'\"\n",
    "        except error as e:\n",
    "            return f\"Error in 'ImageRecognition.size' - unhandled error: {e}\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
